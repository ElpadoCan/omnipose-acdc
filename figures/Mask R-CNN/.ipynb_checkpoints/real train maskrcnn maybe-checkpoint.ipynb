{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2ecbb7-adfe-4497-b52f-159cec73392b",
   "metadata": {},
   "source": [
    "# Mask R-CNN\n",
    "The original Mask R-CNN implenetation is too old to run on one system with modern CUDA drivers etc. (even some forks whcih purport to have updated it for Tensorflow 2.x are not working for me), so I opted to use the `torchvision` version. The goal here is to make it as easy as possible for someone to reproduce this work, and having to deal with the nightmare that is out-of-date tensorflow and CUDA is not tenable. \n",
    "\n",
    "This script adapts a couple key functions (like the data loader and remove_overlaps) from the original train_maskrcnn script from Cellpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd4c2c-d031-4dff-9c53-a2ad08a2d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "import time, os, sys\n",
    "from tifffile import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf4629c-bda5-4c88-8c76-d09d62354e03",
   "metadata": {},
   "source": [
    "### Define data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76968d1-2dfa-46ab-823c-348ff879c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "from cellpose import io, transforms\n",
    "from omnipose.utils import format_labels\n",
    "import skimage.io\n",
    "from tifffile import imread\n",
    "import omnipose\n",
    "\n",
    "class BacteriaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        mask_filter = '_masks'\n",
    "        imf = ''\n",
    "        img_names = io.get_image_files(root,mask_filter,imf=imf,look_one_level_down=True)\n",
    "        mask_names,flow_names = io.get_label_files(img_names, mask_filter, imf=imf)\n",
    "#         self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "#         self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "        self.imgs = list(sorted(img_names))\n",
    "        self.masks = list(sorted(mask_names))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(idx)\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, self.masks[idx])\n",
    "        img = skimage.io.imread(img_path)#.convert('RGB')\n",
    "        img = np.stack([omnipose.utils.normalize99(img*(2**8-1))]*3,axis=-1).astype(uint8)\n",
    "        img = Image.fromarray(img) # must convert to uint8 for pil  \n",
    "        mask = imread(mask_path)\n",
    "        mask = (np.array(mask))\n",
    "        \n",
    "        # instances are encoded as different numbers\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the integer-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            if xmax!=xmin and ymax!=ymin:\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "            else:\n",
    "                print('uh oh',idx,obj_ids[i])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.bool)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        target[\"path\"] = self.masks[idx]\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "#         torch.cuda.empty_cache()\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9825bc09-731c-4e29-80ef-e97fa8466176",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50157ca3-6d18-4a19-aa70-343352e6d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False,pretrained_backbone=True)\n",
    "    \n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "    model.roi_heads.detections_per_img = 1000\n",
    "    model.roi_heads.nms_thresh = 0.7\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc7d090-40e0-4689-bdf1-77f757e2e21a",
   "metadata": {},
   "source": [
    "### Define training augmentations\n",
    "Notably, only random flipping is implemented here, as I beleive was the case for the original Mask R-CNN tensorflow implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7622d6-267c-46a5-b19e-020a0ce10b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44418e5-289b-40db-a70e-daee5e8a1f3b",
   "metadata": {},
   "source": [
    "### Initialize trainign and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08289c-3112-4376-a7eb-85076b919e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error without lowering this\n",
    "num_workers = 0\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "traindir = '/home/kcutler/DataDrive/omnipose_all/train_sorted'\n",
    "testdir = '/home/kcutler/DataDrive/omnipose_all/test_sorted'\n",
    "# traindir = '/home/kcutler/DataDrive/omnipose_maskrcnn/train'\n",
    "# testdir = '/home/kcutler/DataDrive/omnipose_maskrcnn/test'\n",
    "dataset = BacteriaDataset(traindir, get_transform(train=True))\n",
    "dataset_test = BacteriaDataset(testdir, get_transform(train=False))\n",
    "# split the dataset in train and test set\n",
    "# torch.manual_seed(1)\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=num_workers,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=num_workers,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a890235-c748-49d2-9368-e93c041d2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "im,t = dataset[52]\n",
    "im = dataset.__getitem__(2)[0]\n",
    "plt.imshow(im[0])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc7dc9-6e0b-4a55-a2cb-f7924e5e6808",
   "metadata": {},
   "source": [
    "### Define model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925f55f-ef00-40b8-a093-79d60e2d667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec96021-3e7e-4908-817c-ec9d379ade06",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Train\n",
    "Using recommended/default parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc9415-9afd-4e5d-91f2-dcf68f2372b2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=20)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "#     evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad48db48-030b-4713-98a3-6ec68c9f0c4f",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e28ad-e9b6-47e9-9436-81774ba176f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = '/home/kcutler/DataDrive/maskrcnn/bacterialtrain200epochs_1000detections_per_img_v2'\n",
    "torch.save(model, modeldir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a12e0-4a0a-4f79-a350-b3994fb59736",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9612e43-b882-4f39-aeeb-9fe8d40559cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.load(modeldir)\n",
    "\n",
    "# pick one image from the test set\n",
    "img, _ = dataset_test[0]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "# model.roi_heads.detections_per_img = 1000 #not needed \n",
    "model.roi_heads.nms_thresh = 1\n",
    "# model.roi_heads.score_thresh\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d17bcb-7004-47c3-aaeb-db3c8766eda5",
   "metadata": {},
   "source": [
    "### Reconstruct cell masks\n",
    "Mask R-CNN predicts bounding boxes and cell probability within those bounding boxes, along with a confidence score form 0-1. The output is sorted from high to low scores. There are two way to use this information to recontruct labels:\n",
    "1. Loop over all masks and append to a label matrix with incrementing labels. I optimized this a little bit by appending the highest scores last, thereby overwriting low-confidence labels with the higher confidence ones. \n",
    "2. Same as above, but instead of just appending, send any overlapping pixels to the closest mask. The difference here is that proximity determiens the overwriting of pixels rather than confidence. \n",
    "\n",
    "You could imagine some more sophisticated permutations on these themes, but as you will see, the output is too poor to be meaningfully rescued. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2236a5-98d9-4514-b566-e6f549949f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = prediction[0]['scores'].cpu().numpy() # outputs in descending order\n",
    "cutoff = np.percentile(scores,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211b5c7-8bae-47c3-bde6-25f6d2ba69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method (1)\n",
    "\n",
    "im = img.permute(1, 2, 0).byte().numpy()\n",
    "print(im.shape)\n",
    "r = np.zeros(im.shape[:2])\n",
    "l=0\n",
    "\n",
    "print(scores.shape)\n",
    "inds = np.where(scores>cutoff)[0]\n",
    "for j in np.flip(inds):\n",
    "    l += 1\n",
    "    m = (prediction[0]['masks'][j, 0].cpu())>0.9\n",
    "    r[m] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fec5053-386f-42b6-863d-d6b43ad69cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method (2)\n",
    "\n",
    "def remove_overlaps(masks, cellpix, medians):\n",
    "    \"\"\" replace overlapping mask pixels with mask id of closest mask\n",
    "        masks = Nmasks x Ly x Lx\n",
    "    \"\"\"\n",
    "    overlaps = np.array(np.nonzero(cellpix>1.5)).T\n",
    "    print(overlaps.shape)\n",
    "    dists = ((overlaps[:,:,np.newaxis] - medians.T)**2).sum(axis=1)\n",
    "    tocell = np.argmin(dists, axis=1)\n",
    "    masks[:, overlaps[:,0], overlaps[:,1]] = 0\n",
    "    masks[tocell, overlaps[:,0], overlaps[:,1]] = 1\n",
    "\n",
    "    # labels should be 1 to mask.shape[0]\n",
    "    masks = masks.astype(int) * np.arange(1,masks.shape[0]+1,1,int)[:,np.newaxis,np.newaxis]\n",
    "    masks = masks.sum(axis=0)\n",
    "    return masks\n",
    "\n",
    "\n",
    "overlapping_masks = np.stack([np.array((prediction[0]['masks'][j, 0].cpu())>0.9) for j in inds])\n",
    "# overlapping_masks = prediction[0]['masks'].cpu().numpy().squeeze() #the masks is actually a list of nmasks.Ly.Lx\n",
    "len(overlapping_masks),overlapping_masks.shape\n",
    "\n",
    "medians = []\n",
    "for mask in overlapping_masks:\n",
    "    # print(mask.shape)\n",
    "    \n",
    "    ypix, xpix = np.nonzero(mask)\n",
    "    medians.append((np.array([ypix.mean(), xpix.mean()])))\n",
    "\n",
    "labels = np.int32(remove_overlaps(overlapping_masks, overlapping_masks.sum(axis=0), np.array(medians)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7f0e61-7337-45b9-9689-38b3765b100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlapping_masks.sum(axis=0).shape\n",
    "import ncolor\n",
    "plt.imshow(np.hstack((ncolor.label(labels),ncolor.label(r))),interpolation='none')\n",
    "plt.axis('off')\n",
    "# plt.imshow(labels,interpolation='none')\n",
    "print(len(np.unique(labels)),len(np.unique(r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee36f0e9-a1c8-4e18-a0b5-e6dd7decb44a",
   "metadata": {},
   "source": [
    "### Batch process\n",
    "\n",
    "Looks like method (2) is slightly better, so I'll use that for the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8d492e-87e6-4ef3-9142-fcae80b7b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(modeldir)\n",
    "# torch.save(model, modeldir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57efbbbd-eeae-432b-ac76-f5e3905705cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34878ef-8e62-4f31-8bf9-b4e8f227e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def getname(path,suffix='_masks'):\n",
    "    return os.path.splitext(Path(path).name)[0].replace(suffix,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db681fe6-5334-4473-855a-15ba6e244666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "savedir = '/home/kcutler/DataDrive/omnipose_all/Fig1_comparison/maskrcnn'\n",
    "io.check_dir(savedir)\n",
    "\n",
    "for entry in dataset_test:\n",
    "    img = entry[0]\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "    model.roi_heads.nms_thresh = 1\n",
    "    # model.roi_heads.score_thresh\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])\n",
    "        \n",
    "    scores = prediction[0]['scores'].cpu().numpy() # outputs in descending order\n",
    "    cutoff = np.percentile(scores,25)\n",
    "    inds = np.where(scores>cutoff)[0]\n",
    "\n",
    "    \n",
    "    overlapping_masks = np.stack([np.array((prediction[0]['masks'][j, 0].cpu())>0.9) for j in inds])\n",
    "    # overlapping_masks = prediction[0]['masks'].cpu().numpy().squeeze() #the masks is actually a list of nmasks.Ly.Lx\n",
    "    len(overlapping_masks),overlapping_masks.shape\n",
    "\n",
    "    medians = []\n",
    "    for mask in overlapping_masks:\n",
    "        # print(mask.shape)\n",
    "\n",
    "        ypix, xpix = np.nonzero(mask)\n",
    "        medians.append((np.array([ypix.mean(), xpix.mean()])))\n",
    "\n",
    "    labels = np.int32(remove_overlaps(overlapping_masks, overlapping_masks.sum(axis=0), np.array(medians)))\n",
    "\n",
    "    io.imsave(os.path.join(savedir,getname(entry[1]['path'])+'_masks.tif'),labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c42fe-d832-4047-aa0c-693dce6c6fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea4a01d-9760-43c8-8d41-ec4bd585fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry[1]['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1601d012-7bbc-42a4-88e7-27a9176675d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
