{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c44b8da-5cb6-463f-b275-0be5085026fa",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    ".. include:: ../sinebow.rst\n",
    "\n",
    "```\n",
    "{header-2}`Multiple channels`\n",
    "=============================\n",
    "Omnipose inherits the capability of Cellpose to segment based on multi-channel images. We will use this as an opportunity to show how we can run several models at once on the same image(s), in this case comparing Omnipose to Cellpose trained on the cyto2 dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc143f-0aff-4a81-a7ff-d019a84bd127",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# First, import dependencies.\n",
    "import numpy as np\n",
    "import time, os, sys\n",
    "from cellpose import models, core, utils\n",
    "\n",
    "\n",
    "# This checks to see if you have set up your GPU properly.\n",
    "# CPU performance is a lot slower, but not a problem if \n",
    "# you are only processing a few images.\n",
    "use_GPU = core.use_gpu()\n",
    "print('>>> GPU activated? %d'%use_GPU)\n",
    "\n",
    "# for plotting \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa495d2c-1bfd-49ba-9dcc-89615cf48e8f",
   "metadata": {},
   "source": [
    "## Load file\n",
    "This is one of the images from the cyto2 test dataset. Note that it is a good idea to always work with lists, even when the list of images is 1 long. It allows you to reuse your code easily when you do have a larger set of images to process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e351e50b-fa03-473b-bc07-835e8e315b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import skimage.io\n",
    "\n",
    "\n",
    "urls = ['http://www.cellpose.org/static/images/img02.png']\n",
    "files = []\n",
    "for url in urls:\n",
    "    parts = urlparse(url)\n",
    "    filename = os.path.basename(parts.path)\n",
    "    if not os.path.exists(filename):\n",
    "        sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, filename))\n",
    "        utils.download_url_to_file(url, filename)\n",
    "    files.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c13ef5c-8c1a-44d1-9601-48712cde8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [skimage.io.imread(f) for f in files]\n",
    "imgs = [np.stack((im[...,-1],im[...,1])) for im in imgs] # put cytosol in 1st channel, nucleus in 2nd\n",
    "nimg = len(imgs)\n",
    "imgs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8337f9-5ba1-46f4-8302-a21278a274f3",
   "metadata": {},
   "source": [
    "Read in the images from the file list. It's a good idea to display the images before proceeding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd67a3b-435d-45a3-b966-cfee2f29ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellpose import io, transforms \n",
    "\n",
    "# print some infor about the images \n",
    "for i in imgs:\n",
    "    print(i.shape)\n",
    "nimg = len(imgs)\n",
    "print(nimg)\n",
    "\n",
    "plt.figure(figsize=[2]*2) # initialize figure\n",
    "for k in range(len(imgs)):\n",
    "    # img = transforms.move_min_dim(imgs[k]) # move the channel dimension last\n",
    "    imgs[k] = transforms.normalize99(imgs[k],omni=True)\n",
    "    plt.subplot(1,len(files),k+1)\n",
    "    rgb = np.stack((imgs[k][0],imgs[k][1],np.zeros_like(imgs[k][0])),axis=-1)\n",
    "    plt.imshow(rgb) \n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f635d57e-72e7-415b-aea5-73990f9450e2",
   "metadata": {},
   "source": [
    "## Initialize models\n",
    "most recent model traied via \n",
    "python -m cellpose --train --use_gpu --dir /home/kcutler/DataDrive/cyto2/train --mask_filter _masks --n_epochs 4000 --pretrained_model None  --learning_rate 0.1 --diameter 36 --save_every 50 --save_each --omni --verbose --chan 1 --chan2 2 --RAdam --batch_size 16 --img_filter _img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626923fe-1fb0-43f8-a95e-d3716854f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = ['cyto2','cyto2_omni']\n",
    "# L = len(model_name)\n",
    "# model = [models.CellposeModel(gpu=use_GPU, model_type=model_name[i]) for i in range(L)]\n",
    "model_name = ['cyto2','cyto2_omni']\n",
    "L = len(model_name)\n",
    "\n",
    "model = [models.CellposeModel(gpu=use_GPU, model_type=model_name[0]), \n",
    "         models.CellposeModel(gpu=use_GPU, model_type=model_name[1])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469856ad-65c8-4ba7-b371-e2661516c413",
   "metadata": {},
   "source": [
    "## Run segmentation \n",
    "The channels input can be very confusing. In the Cellpose documentation, it is stated that the list `[chan,chan2]` should represent the main channel to segment (`chan`) and the optional nuclear channel (`chan2`). But to train via CLI, `chan` is the \"channel to segment\" and `chan2` is the nuclear channel, and the Cellpose team states the CLI command used to train their model used `--chan 2 --chan2 1`. Because 0 is grayscale and 1,2,3 are R,G,B (1-based indexing, unlike the Python convention) this means that the given training command actually trains with GREEN cytosol and RED nuclei. This might imply that the cyto2_omni model actually is trained 'incorrectly'. \n",
    "\n",
    "On top of this, the downloaded image has blue nuclei and green cytosol, whereas the cyto2 dataset shows cytosol as channel 0 and nuclei as channel 1. So in fact, I should have trained the cyto2_omni model with `--chan 1 --chan2 2`. (Have not yet done this with most recent models...) Keep this in mind as you train you own models. For now, the following shows what channel arguments you need for the provided cyto2 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3e3476-cf5f-49fe-960b-4faeac822b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "chans = [[2,1],[1,2]] # green cytoplasm [2] and red nucleus [1], see above \n",
    "n = range(nimg) \n",
    "\n",
    "# define parameters\n",
    "mask_threshold = [-1,-1,-2] #new model might need a bit lower \n",
    "verbose = 0 # turn on if you want to see more output \n",
    "use_gpu = use_GPU #defined above\n",
    "transparency = True # transparency in flow output\n",
    "rescale= None # give this a number if you need to upscale or downscale your images\n",
    "flow_threshold = 0 # default is .4, but only needed if there are spurious masks to clean up; slows down output\n",
    "resample = False #whether or not to run dynamics on rescaled grid or original grid \n",
    "\n",
    "N = L+1 # three options: pure cellpose, mixed, omnipose, new omnipose\n",
    "omni = [0,1,1]\n",
    "ind = [0,0,1]\n",
    "masks, flows, styles = [[]]*N, [[]]*N, [[]]*N\n",
    "\n",
    "diameter = 33\n",
    "for i in range(N):\n",
    "    masks[i], flows[i], styles[i] = model[ind[i]].eval([imgs[i] for i in n],channels=chans[ind[i]],diameter=diameter,\n",
    "                                                       mask_threshold=mask_threshold[i],\n",
    "                                                       transparency=transparency,flow_threshold=flow_threshold,\n",
    "                                                       omni=omni[i], #toggle omni\n",
    "                                                       resample=resample,verbose=verbose, \n",
    "                                                       cluster=omni[i],\n",
    "                                                       interp=True, tile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dd703c-6a6a-49fc-b9ff-ffb0559d2bcb",
   "metadata": {},
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9976c6b0-281e-4363-85bf-9e52cb26be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellpose import plot\n",
    "import omnipose\n",
    "\n",
    "for idx,i in enumerate(n):\n",
    "    \n",
    "    for k,ki in enumerate(ind):\n",
    "        \n",
    "        print('model is:',model_name[ki],', omni is:',omni[ki])\n",
    "        maski = masks[k][idx]\n",
    "        flowi = flows[k][idx][0]\n",
    "        fig = plt.figure(figsize=(12,5))\n",
    "        # im = transforms.move_min_dim(imgs[i])\n",
    "        # print(im.shape)\n",
    "        plot.show_segmentation(fig, imgs[i], maski, flowi, channels=chans[i], omni=True, bg_color=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd475e91-1ebf-4227-a129-2beef54dcac9",
   "metadata": {},
   "source": [
    "Some comments on the above: Omnipose pre-processes the images slightly differently (see `normalize99`) and therefore the flow is a bit different even with the same model and input image compared to stock cyto2. The `cluster` option helps a lot to get accurate masks with Omnipose in thin regions, but can result in under-segmentation between cells with poorly-defined flow fields. This can be a weakness of Omnipose relative to Cellpose, but as seen in the paper, Omnipose does slightly better than Cellpose of the cyto2 dataset on average, indicating that it does better in some areas and worse in others. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
