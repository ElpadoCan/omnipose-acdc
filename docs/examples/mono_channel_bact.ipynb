{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c44b8da-5cb6-463f-b275-0be5085026fa",
   "metadata": {},
   "source": [
    "# Basic Omnipose tutorial\n",
    "This notebook demonstrates how to load images, display them, segment them using Omnipose, and visualize both the segmentation results and the intermediate network output. Here we show the details behind the most typical workflow: single-channel segmentation. The bacterial images used are (1) from my own image library and (2-5) from the DeLTA2.0 paper. From the latter, we shall see how to handle images that are intrinsically grayscale but were exported and published as RGB(A) - *i.e.*, there is no extra information in those extra channels to aid segmentation. For two-channel segmentation, see the `multi_channel_cyto` notebook. \n",
    "\n",
    "Before running this notebook, install the latest version of Omnipose from GitHub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc143f-0aff-4a81-a7ff-d019a84bd127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make local editable packages automatically reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import dependencies\n",
    "import numpy as np\n",
    "from cellpose import models, core\n",
    "\n",
    "# This checks to see if you have set up your GPU properly.\n",
    "# CPU performance is a lot slower, but not a problem if you \n",
    "# are only processing a few images.\n",
    "use_GPU = core.use_gpu()\n",
    "print('>>> GPU activated? %d'%use_GPU)\n",
    "\n",
    "# for plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa495d2c-1bfd-49ba-9dcc-89615cf48e8f",
   "metadata": {},
   "source": [
    "### How to load your images\n",
    "There are several ways to load your image files into a notebook. If you have a specific set of images, put their full paths into a list. For example:\n",
    "```python\n",
    "# make it a list even if there is only one file\n",
    "files = ['path_to_image_1'] \n",
    "files = ['path_to_image_1','path_to_image_2']\n",
    "\n",
    "# you can also add to the list like so:\n",
    "files = files + ['path_to_image_3']\n",
    "```\n",
    "\n",
    "Alternatively, you can load all the images in a directory. Here are a few templates you can use to get the list of directories automatically by searching for image files matching a cetrain file lane with an extension and keywords in the file name. \n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "basedir = '<path_to_image_folder>'\n",
    "# use rglob to search subfolders recursively \n",
    "files = [str(p) for p in Path(basedir).rglob(\"*.tif\")] \n",
    "# change the search string to grab only one channel\n",
    "files = [str(p) for p in Path(basedir).glob(\"*C1.png\")]\n",
    "# specify a match anywhere in the file name\n",
    "files = [str(p) for p in Path(basedir).glob(\"*488*.png\")] \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4a895-6934-471e-9913-1b5ccf32e987",
   "metadata": {},
   "source": [
    "We can also use the `cellpose.io` library to grab all the images in the test_files folder. This is very handy for grabbing images of different extensions. Here we are using  four RGB(A) images from the DeLTA 2.0 training set (on which the bact_phase_omni model has never been trained) as well as an RGB image acquired in the same lab as much of the Omnipose `bact_phase` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1452d605-54d0-417e-a720-33436e319231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from cellpose import io\n",
    "\n",
    "basedir = os.path.join(Path.cwd().parent,'test_files')\n",
    "files = io.get_image_files(basedir)\n",
    "files # this displays the variable if it the last thing in the code block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8337f9-5ba1-46f4-8302-a21278a274f3",
   "metadata": {},
   "source": [
    "Next we read in the images from the file list. It's a good idea to display the images before proceeding. Here I happen to be reading in some RBG tiles of grayscale phase contrast images (such as you might use for figures etc.) as well as some single-channel images. As part of the visualization process, the images are rescaled to be in the range 0-1. Omnipose does this exact thing internally (you don't have to rescale them prior to running segmentation via CLI). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd67a3b-435d-45a3-b966-cfee2f29ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellpose import io, transforms\n",
    "from omnipose.utils import normalize99\n",
    "imgs = [io.imread(f) for f in files]\n",
    "\n",
    "# print some info about the images.\n",
    "for i in imgs:\n",
    "    print('Original image shape:',i.shape)\n",
    "    print('data type:',i.dtype)\n",
    "    print('data range:', i.min(),i.max())\n",
    "nimg = len(imgs)\n",
    "print('number of images:',nimg)\n",
    "\n",
    "fig = plt.figure(figsize=[40]*2) # initialize figure\n",
    "print('new shape:')\n",
    "for k in range(len(imgs)):\n",
    "    img = transforms.move_min_dim(imgs[k]) # move the channel dimension last\n",
    "    if len(img.shape)>2:\n",
    "        # imgs[k] = img[:,:,1] # could pick out a specific channel\n",
    "        imgs[k] = np.mean(img,axis=-1) # or just turn into grayscale \n",
    "        \n",
    "    imgs[k] = normalize99(imgs[k])\n",
    "    print(imgs[k].shape)\n",
    "    plt.subplot(1,len(files),k+1)\n",
    "    plt.imshow(imgs[k],cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f635d57e-72e7-415b-aea5-73990f9450e2",
   "metadata": {},
   "source": [
    "Note that the first two images are RGB, the third and fifth are mono-channel, and the fourth is RGBA (the alpha channel encodes transparency). Exporting to RGB is usually just done for making diagrams or making images compatible with non-scientific viewing software. Pro tip: Adobe Illustrator *will not* interpolate the pixels in your image if you save it as RGB, but it *will* if you keep it mono-channel. Usually you want exact, non-interpolated (pixelated) images to be presented since it is your raw data, so you can convert it to grayscale by `im_RGB = [im,im,im]` (or more slick, `im_RGB = [im,]*3` or `*4` for RGBA). However, storing *all* your images this way is a waste of space - just do it for the ones you need for a figure. \n",
    "\n",
    "Also note that the DeLTA images (1-4) are uint8, so `0` to `2**8-1 = 255`. Image 1 only takes up values in the range 4 to 22 out of a possible 0 to 255, meaning it was probably way too dark and not rescaled prior to conversion to an 8-bit image. In my experience, images are typically 14-bit (that depends on your camera) and therefore saved as 16-bit lossless formats like PNG or TIF (Omnipose can detect and segment JPEGs, but you would never use those for anything scientific, even for figures due to compression artifacts). Using only `22-4 = 18` levels of gray to depict the cells causes the distinct 'posterized' effect that you can see if you zoom up on the image. \n",
    "\n",
    "\n",
    "### Initialize \n",
    "Here we use one of the built-in model names. You can print out the available model names, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f645d6-c2cc-45bb-80da-22efa8cb4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellpose import models\n",
    "from cellpose.models import MODEL_NAMES\n",
    "\n",
    "MODEL_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79b2197-6f09-4123-922a-b64947f1c45f",
   "metadata": {},
   "source": [
    "We will choose the `bact_phase_omni` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af70ccdc-d166-47bd-b797-f3330f72dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bact_phase_omni'\n",
    "model = models.CellposeModel(gpu=use_GPU, model_type=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469856ad-65c8-4ba7-b371-e2661516c413",
   "metadata": {},
   "source": [
    "### Run segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ad2e8b-59eb-49a2-90b3-fc1407ed9cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "chans = [0,0] #this means segment based on first channel, no second channel \n",
    "\n",
    "n = [0] # make a list of integers to select which images you want to segment\n",
    "n = range(nimg) # or just segment them all \n",
    "\n",
    "# define parameters\n",
    "mask_threshold = -1 \n",
    "verbose = 0 # turn on if you want to see more output \n",
    "use_gpu = use_GPU #defined above\n",
    "transparency = True # transparency in flow output\n",
    "rescale=None # give this a number if you need to upscale or downscale your images\n",
    "omni = True # we can turn off Omnipose mask reconstruction, not advised \n",
    "flow_threshold = 0 # default is .4, but only needed if there are spurious masks to clean up; slows down output\n",
    "resample = True #whether or not to run dynamics on rescaled grid or original grid \n",
    "\n",
    "masks, flows, styles = model.eval([imgs[i] for i in n],channels=chans,rescale=rescale,mask_threshold=mask_threshold,transparency=transparency,\n",
    "                                  flow_threshold=flow_threshold,omni=omni,resample=resample,verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dd703c-6a6a-49fc-b9ff-ffb0559d2bcb",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff61688-f481-4a0b-9012-4f710d814a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellpose import plot\n",
    "import omnipose\n",
    "\n",
    "for idx,i in enumerate(n):\n",
    "\n",
    "    maski = masks[idx]\n",
    "    flowi = flows[idx][0]\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    # plot.show_segmentation(fig, omnipose.utils.normalize99(imgs[i][:,:,0:2]), maski, flowi, channels=chans, omni=True, bg_color=0)\n",
    "    plot.show_segmentation(fig, omnipose.utils.normalize99(imgs[i]), maski, flowi, channels=chans, omni=True, bg_color=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9190bc2-58c2-4b32-b3a3-ee85e95716c9",
   "metadata": {},
   "source": [
    "### Save the results\n",
    "Often you will want to save your masks before moving on to analysis (that way to can just load them in instead of re-running segmentation). I improved the `cellpose.io` function quite a bit to be more flexible in where it can save. See the documentation page for the full list of options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73d5c2-6a5a-41f4-914b-6acee17f7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.save_masks(imgs, masks, flows, files, \n",
    "              png=False,\n",
    "              tif=True, # whether to use PNG or TIF format\n",
    "              suffix='', # suffix to add to files if needed \n",
    "              save_flows=False, # saves both RGB depiction as *_flows.png and the raw components as *_dP.tif\n",
    "              save_outlines=False, # save outline images \n",
    "              dir_above=0, # save output in the image directory or in the directory above (at the level of the image directory)\n",
    "              in_folders=True, # save output in folders (recommended)\n",
    "              save_txt=False, # txt file for outlines in imageJ\n",
    "              save_ncolor=False) # save ncolor version of masks for visualization and editing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3114808e-52c3-4ac5-b8bc-8ad08fadaec2",
   "metadata": {},
   "source": [
    "### Debug results\n",
    "\n",
    "The RGB flows shown above will give you some insight as to if there is an issue with the flow field outputs, but you can also check out the boundary and distance output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920de0d3-fefc-4e0d-ac11-286b20976905",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,i in enumerate(n):\n",
    "\n",
    "    disti = flows[idx][2]\n",
    "    bdi = flows[idx][4]\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    plt.imshow(np.hstack([disti,bdi]))\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afec6ab-3956-4898-8ef9-7b768b067951",
   "metadata": {},
   "source": [
    "### Notes on the above\n",
    "The distance field is trained with background pixels set to `-5` in older models and `-<mean diameter>` in newer models. This helps to make the desired network output more balanced and give more distinction between edge pixels (which have values close to 0) and background (which ordinarily would have a value of 0). The flow field, being the gradient of the distance field, *by definition* has a magnitude of 1 everywhere - but we rescale it by `5` for training. This helps by bringing the desired flow component output more in the range of the boundary output, which is the input to the sigmoid function (so-called 'logits') and therefore ranges from about `-5` to `5`. \n",
    "\n",
    "What you can see in the images above is that the features in the boundary, distance, and flow fields are all very consistent with each other. For example, the flow field has positive divergence where the boundary output is high and negative divergence where the distance field is high. This is by design, as I included the boundary field for the sole purpose of improving the prediction accuracy on the flow and distance fields. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
