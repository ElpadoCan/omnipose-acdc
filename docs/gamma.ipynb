{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1d74023-6f9c-4ece-a7ce-c74a6879edc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "```{eval-rst}\n",
    ".. include:: sinebow.rst\n",
    "\n",
    "```\n",
    "{sinebow22}`Gamma`\n",
    "=================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f001c-90ad-487b-8875-4d60658d4e39",
   "metadata": {},
   "source": [
    "One of the more trivial uses of good binary segmentation (let alone best-in-class *instance* segmentation) is the ability to adjust an image based on foreground/background values. \n",
    "\n",
    "{header-2}`Example Image`\n",
    "--------------------------\n",
    "To start off, take our example image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5305e668-ed49-40b0-8d27-f125ed1e98e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')  # some plot elements will not be visible in light mode\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from cellpose import io\n",
    "im = io.imread('test_files/e1t1_crop.tif')\n",
    "\n",
    "plt.imshow(im,cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53caf9e0-103b-441e-acc2-e670bc716a1f",
   "metadata": {},
   "source": [
    "This image is 16-bit and already adjusted to span the entire bit depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb33e0d9-3d5e-4abf-b492-44b92dc8cdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(im.dtype, im.ptp()==(2**16-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37765468-6ddb-4bca-9b8d-a7bb97e65ba1",
   "metadata": {},
   "source": [
    "{header-2}`Exposure and outliers`\n",
    "----------------------------------\n",
    "\n",
    "Raw data is often under- or over-exposed and can contain outliers where pixels are saturated. We can simulate this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d14041-567b-447f-8f16-be55675ac834",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_bad = im * .5 # reduce brightness by 50%\n",
    "print('50% image intensity (no apparent change):')\n",
    "plt.imshow(im_bad,cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "im_bad[im.shape[0]//3,im.shape[1]//5] = im_bad.max()*2 # add a bright pixel \n",
    "print('Outlier pixel makes everything else look dark:')\n",
    "plt.imshow(im_bad,cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ca9984-640c-4eab-9ab0-a8acf1ccfe00",
   "metadata": {},
   "source": [
    "The `plt.imshow` command simply maps the minimum value of the image to 0 and the maximum value of the image to 1, i.e. it applies standard *0-1 min-max normalization*. This explains the dark appearance once we add in a bright pixel, as most of the image gets mapped to the bottom half of the available color map.\n",
    "\n",
    "This is annoying when visualizing images next to each other, but it is particularly problematic when we need to standardize the images we feed into a neural network. We can choose to make all images 0-1, 0-255, etc. (and these can go above or below the minimum and maximum by a little), but it is much harder for a network to learn foreground from background if the images are chaotically rescaled like the above example (chaotic meaning that the image darkening is highly sensitive to the particular condition of whether or not there are saturated pixels). \n",
    "\n",
    "We solve this by normalizing the image not by the absolute min and max, but by percentiles. We set pixels at or below the 0.01 percentile to 0 and those at or above the 99.99th percentile to 1. (Cellpose uses 1 and 99, but this will mess up images with very few cells compared to background). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d72d8-07e2-4038-b14c-9ab9ab3b87cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnipose.utils import normalize99\n",
    "\n",
    "im_fixed = normalize99(im_bad)\n",
    "print('normalize99() fixes the image:')\n",
    "plt.imshow(im_fixed,cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f411afd-a53c-4e7e-acd8-56a1ed4de110",
   "metadata": {},
   "source": [
    "With an image that has been properly normalized from 0 to 1, we can further adjust it. Right now we cannot see a lot of detail in the dark parts of the image; what we can do is raise the image to some power, called **gammma adjustment**. Because $0^x = 0$ and $1^x = 1$, we can make the image globally brighter or darker without affecting the total range:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98588bf-6367-42f0-959a-9841ba0bd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from omnipose.utils import sinebow\n",
    "\n",
    "im_gamma = []\n",
    "gamma = [0.25, 0.5, 1, 2]\n",
    "N = len(gamma)\n",
    "\n",
    "darkmode = 1\n",
    "if darkmode:\n",
    "    plt.style.use('dark_background')\n",
    "    axcol = 'w'\n",
    "    background_color = 'k'\n",
    "else:\n",
    "    mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "    axcol = 'k'\n",
    "    background_color = np.array([1,1,1,1])\n",
    "\n",
    "    \n",
    "sz = 8\n",
    "labelsize = 7\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "mpl.rcParams[\"axes.facecolor\"] = [0,0,0,0]\n",
    "fig,ax = plt.subplots(figsize=(sz,sz/N),facecolor='#0000')\n",
    "\n",
    "color = sinebow(N+1)\n",
    "for j,g in enumerate(gamma):\n",
    "    i = im_fixed**g\n",
    "    im_gamma.append(i)\n",
    "    plt.hist(i.flatten(),\n",
    "             bins=100,\n",
    "             label='gamma = {}'.format(g),\n",
    "             color=color[j+1],\n",
    "             histtype='step',\n",
    "             density=True)\n",
    "\n",
    "l = plt.legend(prop={'size': labelsize},frameon=False)\n",
    "for text,c in zip(l.get_texts(),[color[i] for i in range(1,N+1)]):\n",
    "    text.set_color(c)\n",
    "for item in l.legendHandles:\n",
    "    item.set_visible(False)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.patch.set_alpha(0.0)\n",
    "plt.xlabel('Intensity')\n",
    "plt.ylabel('PDF')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "imstack = np.hstack(im_gamma)\n",
    "sz = np.array(imstack.shape)\n",
    "fig =  plt.figure(figsize=sz/sz[0]*5,facecolor='#0000')\n",
    "\n",
    "imstack = np.stack([imstack]*4,axis=-1)\n",
    "s = im.shape\n",
    "pad = 10\n",
    "width = 30\n",
    "for j in range(1,N+1):\n",
    "    slc = (slice(pad,pad+width),slice(s[1]*j-(pad+width),s[1]*j-pad),Ellipsis)\n",
    "    imstack[slc] = color[j]\n",
    "\n",
    "\n",
    "plt.imshow(imstack)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ded48f-f658-4684-a70f-1aa312dc3963",
   "metadata": {},
   "source": [
    "Note that fractional powers only work (without going into complex numbers) if the image values are nonnegative. \n",
    "\n",
    "\n",
    "{header-2}`Semantic gamma normalization`\n",
    "-----------------------------------------\n",
    "We can next use image segmentation in combination with gamma adjustment to normalize image brightness. This is very handy for making figures with images coming from different microscopes or optical configurations. To demonstrate, let's load in the image set from our *mono_channel_bact* notebook and the corresponding masks we made with Omnipose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fd464a-1441-494b-b739-feb0887f82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from cellpose import io, transforms\n",
    "\n",
    "basedir = os.path.join(Path.cwd().parent,'docs','test_files')\n",
    "mask_filter='_cp_masks'\n",
    "img_names = io.get_image_files(basedir,mask_filter,look_one_level_down=True)\n",
    "mask_names = io.get_label_files(img_names,subfolder='masks')\n",
    "imgs =  [io.imread(i) for i in img_names]\n",
    "masks = [io.imread(m) for m in mask_names]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251f6396-b94d-48b9-a82d-369ebcedfb63",
   "metadata": {},
   "source": [
    "Now we will compare standard normalization to what I am calling \"semantic gamma normalization\". My implementation of it can be found in `omnipose.utils`, which simply answers the question: \"what is the power to which I need to raise my image such that the average background becomes equal to a given value?\". From left to right, I plot `im/max(im.dtype)` (so min>=0 and max=1), 0-1 remapping of `im` (`recsale`), percentile remapping of `im` (`normalize99`), gamma normalization to background of 1/3, and gamma normalization of background to 1/2. The output has been set to use the same colormap and interpolation (`vmin` and `vmax` are otherwise set by the min and max of the image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f098dd9-1b59-4208-bcc4-759061d74c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnipose.utils import rescale, normalize_image\n",
    "\n",
    "for im, mask in zip(imgs,masks):\n",
    "    \n",
    "    # format the image\n",
    "    im = transforms.move_min_dim(im) # move the channel dimension last\n",
    "    if len(im.shape)>2:\n",
    "        im = im[:,:,1] \n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(20,10),facecolor='#0000')\n",
    "    im_rescale = rescale(im)\n",
    "    im_norm = normalize99(im)\n",
    "    im_gamma_3 = normalize_image(im, mask>0, bg=1/3)\n",
    "    im_gamma_2 = normalize_image(im, mask>0, bg=1/2)\n",
    "    \n",
    "    plt.imshow(np.hstack([im/np.iinfo(im.dtype).max,im_rescale,im_norm,im_gamma_3, im_gamma_2]),vmin=0,vmax=1, cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6fd12-7c5d-40c2-9b9e-cdc90e42931a",
   "metadata": {},
   "source": [
    "The first column provides an essentially 'raw' view of the image, as it has not been shifted or stretched relative to the original max and min of its data type. As noted in the segmentation notebook, that first image is super dark because it is an 8-bit image, 0-255, but only takes on values from 4 to 22. My code above divides by 255 for uint8 images and 65535 for the last uint16 image. \n",
    "\n",
    "The second and third columns do stretch the image to fill the whole 0-1 range, but you can see how the images still have different background intensity. My function in columns 4 and 5 normalize the background to a constant value. Well-exposed bacterial phase contrast images seem to have a 'natural' background value of about 1/3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02f4e2-e079-47f5-80a9-596cabd2e0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
